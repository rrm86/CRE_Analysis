{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' O projeto se divide em análise de timesires e uma tentativa de prever evazão\n",
    "fazendo uso de um algoritmo supervisionado\n",
    "----Passos Time series\n",
    "1 - Baixar o Log no moodle de eventos no moodle para atividades de participação -\n",
    "OK\n",
    "2 - Filtrar os usuário que não são alunos - Excel -pré processamento\n",
    "OK\n",
    "3 - Fazer discretização(IP) - Coluna Rede_Puc\n",
    "OK\n",
    "4 - Adicionar uma coluna referente ao cronograma(unidade 1 e etc)\n",
    "Ok\n",
    "5 - Adicionar coluna com as turmas\n",
    "OK\n",
    "6-  Plotar gráficos com a visualização\n",
    "OK\n",
    "------\n",
    "------ Passos Forecasting\n",
    "1 - Converter a tabela de time series em um data set anterior em um transicional\n",
    "    1.1 - Plotar gráfico com estatśticas gerais de uso -OK\n",
    "    1.2 - Talvez clusterizar os alunos - OK\n",
    "2 - Descobrir quais usuários trancaram a disciplina - Log de atividades CLI\n",
    "OK\n",
    "3 - Descobrir se existe uma data com pico de trancamento\n",
    "OK\n",
    "4 - Baseado na relação entre unidade/trancamento definir um intervalo e filtrar o dataset antes da data de interesse.\n",
    "(vai ser os usuários que trancaram durante a unidade 3 e unidade 4 )\n",
    "OK \n",
    "\n",
    "5- Mesclar com o dataset dos usuários. Adicionar mais uma coluna(trancou[1,0])\n",
    "OK\n",
    "6 - Alimentar random forest\n",
    "OK\n",
    "7 - avaliar desempenho\n",
    "OK\n",
    "8 - extrair regras\n",
    "'''\n",
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from datetime import datetime        \n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time series att\n",
    "def rede_puc(turmas):\n",
    "    ip = '139.82'\n",
    "    rede = []\n",
    "    for x in turmas.IP:\n",
    "        if(ip == x[0:6]):\n",
    "            rede.append('S')\n",
    "        else:\n",
    "            rede.append('N')\n",
    "    turmas.insert(1, column='Rede_Puc', value=rede)#Passo 5\n",
    "    return turmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_turmas():\n",
    "    df_ = pd.DataFrame()\n",
    "    frames = [df_]\n",
    "    \n",
    "    for dados_turma in glob.glob('dados/turmas/*'):\n",
    "        grupo = dados_turma[13:-4]# obtem código do grupo\n",
    "        df = pd.read_csv(dados_turma)#lê arquivo\n",
    "        df.insert(3, column='Turma', value=grupo)#insere coluna\n",
    "        print(df.shape)\n",
    "        frames.append(df)\n",
    "    dados = pd.concat(frames)\n",
    "\n",
    "    dados = normaliza_data(dados)\n",
    "    \n",
    "    \n",
    "    return dados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliza_data(dados):\n",
    "    dados.columns = [c.replace(' ', '_') for c in dados.columns]#Troca espaço por underline no cabeçalho\n",
    "    dados.drop_duplicates(keep=False, inplace=True)#limpando dados duplicados\n",
    "    dados['Data'], dados['Hora'] = dados['Hora'].str.split(' ', 1).str#divide a coluna Hora em duas colunas\n",
    "    dados.Data = pd.to_datetime(dados.Data,dayfirst=True)\n",
    "    return dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time series att\n",
    "def obter_unidades(t): \n",
    "    unidades= []\n",
    "    data_unidades = {\n",
    "        'IU1':datetime(2018, 3, 19),\n",
    "        'FU1':datetime(2018, 4, 18),\n",
    "        'IU2':datetime(2018, 4, 18),\n",
    "        'FU2':datetime(2018, 5, 9),\n",
    "        'IU3':datetime(2018, 5, 9),\n",
    "        'FU3':datetime(2018, 5, 23),\n",
    "        'IU4':datetime(2018, 5, 23),\n",
    "        'FU4':datetime(2018, 6, 27)\n",
    "    }\n",
    "\n",
    "    for x in t.Data:\n",
    "        if(data_unidades['IU1'] <= x < data_unidades['FU1']):\n",
    "            unidades.append('unidade_1')\n",
    "        elif(data_unidades['IU2'] <= x < data_unidades['FU2']):\n",
    "            unidades.append('unidade_2')\n",
    "        elif(data_unidades['IU3'] <= x < data_unidades['FU3']):\n",
    "            unidades.append('unidade_3')\n",
    "        elif(data_unidades['IU4'] <= x < data_unidades['FU4']):\n",
    "            unidades.append('unidade_4')\n",
    "        else:\n",
    "            unidades.append('fora_de_epoca')\n",
    "            \n",
    "        \n",
    "    t.insert(1, column='Unidade', value=unidades)\n",
    "    return t\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_periodo_dia(t):\n",
    "    periodo = {\n",
    "        'P1I':'00:00',\n",
    "        'P1F':'04:00',\n",
    "        'P2I':'04:00',\n",
    "        'P2F':'08:00',\n",
    "        'P3I':'08:00',\n",
    "        'P3F':'12:00',\n",
    "        'P4I':'12:00',\n",
    "        'P4F':'16:00',\n",
    "        'P5I':'16:00',\n",
    "        'P5F':'20:00',\n",
    "        'P6I':'20:00',\n",
    "        'P6F':'24:00'\n",
    "    }\n",
    "    periodo_list = []\n",
    "    for x in t.Hora:\n",
    "        if (periodo['P1I'] <= x < periodo['P1F']):\n",
    "            periodo_list.append('1')\n",
    "        elif(periodo['P2I'] <= x < periodo['P2F']):\n",
    "            periodo_list.append('2')\n",
    "        elif(periodo['P3I'] <= x < periodo['P3F']):\n",
    "            periodo_list.append('3')\n",
    "        elif(periodo['P4I'] <= x < periodo['P4F']):\n",
    "            periodo_list.append('4')\n",
    "        elif(periodo['P5I'] <= x < periodo['P5F']):\n",
    "            periodo_list.append('5')\n",
    "        elif(periodo['P6I'] <= x < periodo['P6F']):\n",
    "            periodo_list.append('6')\n",
    "\n",
    "        \n",
    "    t.insert(1, column='Periodo', value=periodo_list)\n",
    "    return t\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turmas = obter_turmas() \n",
    "turmas = rede_puc(turmas)\n",
    "turmas = obter_unidades(turmas)\n",
    "turmas = obter_periodo_dia(turmas)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "turmas.to_csv('turmas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "turmas['Unidade'].value_counts().sort_index().plot.bar(figsize=(24, 12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(turmas['Unidade']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turmas['Periodo'].value_counts().sort_index().plot.bar(figsize=(24, 12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=turmas['Periodo'],data=turmas);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    recursos = turmas['Contexto_do_Evento'].value_counts().plot.bar(figsize=(24, 12));\n",
    "recursos.set_title(\"Recursos\", fontsize=20);\n",
    "turmas['Contexto_do_Evento'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turmas['Turma'].value_counts().sort_index().plot.bar(figsize=(24, 12));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=turmas['Turma'],data=turmas);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turmas['Rede_Puc'].value_counts().sort_index().plot.bar(figsize=(24, 12));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parte 2\n",
    "turmas_pre_clean=turmas.groupby('Nome_completo')['Contexto_do_Evento'].value_counts().sort_index().unstack().fillna(0)#tranformando timesires em data frame\n",
    "turmas_pre_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turmas_pre_clean.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import decomposition\n",
    "def cluster(t):\n",
    "    data=t\n",
    "    kmeans = KMeans(n_clusters = 8, init = 'k-means++')\n",
    "    kmeans = kmeans.fit(data)\n",
    "    plot_2D_cluster(data,kmeans)\n",
    "    plot_3D_cluster(data,kmeans)\n",
    "    return kmeans\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2D_cluster(data,kmeans):\n",
    "    pca = decomposition.PCA()\n",
    "    pca.n_components = 2\n",
    "    X_reduced = pca.fit_transform(data)\n",
    "    values = kmeans.cluster_centers_.squeeze()\n",
    "    labels = kmeans.labels_\n",
    "    centroid = pca.fit_transform(values)\n",
    "    centroid_label = set(labels) \n",
    "    plt.scatter(X_reduced[:,0],X_reduced[:,1],c=labels,cmap='tab20')\n",
    "    #plt.scatter(centroid[:,0],centroid[:,1],c=range(0,len(centroid_label)),cmap='Set1',marker='D')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "def plot_3D_cluster(data,kmeans):\n",
    "    fig = plt.figure()\n",
    "    ax = Axes3D(fig)\n",
    "    \n",
    "    pca = decomposition.PCA()\n",
    "    pca.n_components = 3\n",
    "    X_reduced = pca.fit_transform(data)\n",
    "    values = kmeans.cluster_centers_.squeeze()\n",
    "    labels = kmeans.labels_\n",
    "    centroid = pca.fit_transform(values)\n",
    "    centroid_label = set(labels) \n",
    "    \n",
    "    ax.scatter(X_reduced[:,0],X_reduced[:,1],X_reduced[:,1] , c=labels,cmap='tab20')\n",
    "    #ax.scatter(centroid[:,0],centroid[:,1],centroid[:,2],c=range(0,len(centroid_label)),cmap='Set1',marker='D')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_trancamentos():\n",
    "    trancamento = pd.read_csv('dados/trancamento.csv')\n",
    "    trancamento = normaliza_data(trancamento)\n",
    "    filtro = trancamento[trancamento.Data < '2018-06-27 08:00:00']#filtra a data\n",
    "    return filtro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = cluster(turmas_pre_clean.iloc[:, :,].values)\n",
    "#3-5-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turmas_pre_clean.to_csv('alunos_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alunos_clean = pd.read_csv('alunos_clean.csv')\n",
    "alunos_clean.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slugify import slugify\n",
    "def obter_log_evadidos (evasao):\n",
    "    # Remove usuários que trancaram a disciplina no periodo fora de interesse\n",
    "    target = evasao[(evasao.Unidade == 'unidade_3') | (evasao.Unidade == 'unidade_4')]\n",
    "    target.sort_values(by='Data')\n",
    "    id_alunos = []\n",
    "    for i in target['Evento']:\n",
    "        id_alunos.append(i[-36:-31])#usados na query\n",
    "        \n",
    "    alunos_evadidos =  pd.read_csv('dados/trancamento_clean.csv')\n",
    " \n",
    "    return alunos_evadidos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe = alunos_clean.describe()\n",
    "describe.to_csv('metricas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evasao = obter_trancamentos()\n",
    "evasao = obter_unidades(evasao)\n",
    "evasao.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evasao['Unidade'].value_counts().sort_index().plot.bar(figsize=(24, 12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_evadidos = obter_log_evadidos(evasao)\n",
    "\n",
    "log_evadidos.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_evadidos['component'].value_counts().sort_index().plot.bar(figsize=(24, 12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evadidos_data=log_evadidos.groupby('userid')['component'].value_counts().sort_index().unstack().fillna(0)#tranformando timesires em data frame\n",
    "evadidos_data.to_csv('evadidos_count.csv')\n",
    "evadidos_data = pd.read_csv('evadidos_count.csv')\n",
    "print(len(evadidos_data))\n",
    "print(len(alunos_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_alunos(evadidos, alunos_clean):\n",
    "    #evadidos.rename(columns={'userid':'Nome_completo'}, inplace=True)\n",
    "    evadidos.drop(['userid'], axis=1)\n",
    "    alunos_clean.drop(['Nome_completo'], axis=1)\n",
    "    \n",
    "    alunos_clean['Trancou'] = 0\n",
    "    evadidos['Trancou'] = 1\n",
    "    return pd.concat([alunos_clean, evadidos_data],join=\"inner\",sort=True,ignore_index=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evadidos_data.head()\n",
    "alunos_final = merge_alunos(evadidos_data, alunos_clean)\n",
    "alunos_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_finais = alunos_final.drop(['Trancou'], axis=1)\n",
    "targets_finais = alunos_final['Trancou']\n",
    "targets_finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster(dados_finais.values);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dados_finais.values, \n",
    " targets_finais.values, \n",
    " test_size = 0.4, \n",
    " random_state = 0)\n",
    "# Show the results of the split\n",
    "print(\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print(\"Testing set has {} samples.\".format(X_test.shape[0]))\n",
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "rf = RandomForestClassifier(random_state=0, n_estimators = 200, class_weight=\"balanced\")\n",
    "model = rf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = rf.predict(X_test)\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "\n",
    "precision, recall, fscore, support = score(y_test, resultado)\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "# Pull out one tree from the forest\n",
    "feature_list = list(dados_finais.columns)\n",
    "tree = rf.estimators_[5]\n",
    "# Export the image to a dot file\n",
    "export_graphviz(tree, out_file = 'tree3.dot', feature_names = feature_list, rounded = True, precision = 1)\n",
    "# Use dot file to create a graph\n",
    "(graph, ) = pydot.graph_from_dot_file('tree3.dot')\n",
    "# Write graph to a png file\n",
    "#graph.write_png('tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph.write_png('tree.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_finais.columns\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
